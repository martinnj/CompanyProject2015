\subsection{Evaluation}
\label{sec:eval}

In the evaluation phase of the CRISP-DM process, the model(s) produced in the
earlier step is carefully examined and evaluated; i.e. does it solve the
business problem? By the end of this phase a decision should be made wether the
data mining results can be used, or if more iteration is needed. Even if the
model is deployed, it is still possible to continue the process of iteration to
create a better model or discover more things about the data or business
problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Dataset Bias}

While running the tests that produced the data in Table \ref{tab:formulacompare}
I noticed that while the accuracy stayed around $93 - 94$ \% for all the runs,
the model did not have the same precision when predicting \texttt{TRUE} as it
does when predicting \texttt{FALSE}. The model would be wrong when predicting a
customer to be \textit{retained} one time out of three, giving it an accuracy
for predicting \texttt{TRUE} of around $66.666$ \%. When guessing \texttt{FALSE}
on the other hand; the model would have an accuracy of around $95$ \% of the
time. The data for these calculations can be seen in Table
\ref{app:tab:confusion1}, Table \ref{app:tab:confusion2} and Table
\ref{app:tab:confusion3} in Appendix \ref{app:confusion}.

Looking at the datasets it looks like this trend is caused by the number of
observations for each class. Both the training and test dataset contains more
than ten times as many unretained customers as retained, the exact numbers can
be seen in Table \ref{tab:datasetretention}. This essentially returned me to the
data understanding phase of the CRISP-DM process, but for the sake of continuity
in this reportm the data preparation, modelling and evaluation of this extra
iteration will be written in this section.

\begin{table}[H]
  \centering
  \begin{tabular}{lll}
    \textbf{Dataset}  & \texttt{TRUE} & \texttt{FALSE} \\ \hline
    \textit{Training} & $30358$       & $433358$       \\
    \textit{Test}     & $40731$       & $454659$       \\
    \textit{Equal}    & $30358$       &  $30358$
  \end{tabular}
  \caption{The distribution of the \textit{iscjretained} target variable classes
    in the different datasets.}
  \label{tab:datasetretention}
\end{table}

In order to try and combat the bias in the dataset I constructed a new dataset I
will refer to as dataset \textit{equal}. This new dataset contains all of the
observations from the training dataset who is \textit{retained}, and a random
sample, of the same size, of the observations who is not.

Like in Section \ref{sec:formcompare} I created a new set of tests (the source
code for these tests is available in Figure \ref{app:code:equal} in Appendix
\ref{app:equal}) which would test the new accuracy of the model with varying
tree depths. Using this new test, the overall accuracy for drops to around $82$
\% ($81.6753$ \%, $81.9569$ \% and $81.9438$ \%) for maximum depths of 4, 6 and
8, for the detailed numbers, please refer to Table \ref{app:tab:confusion11},
Table \ref{app:tab:confusion22} and Table \ref{app:tab:confusion33} in Appendix
\ref{app:confusion}.

The new dataset and model does better at guessing retained customers with a mean
accuracy of around $82$ \% for both \texttt{TRUE} and \texttt{FALSE}. While this
increase in accuracy for retained customers is roughly equal to the decrease in
accuracy for unretained customers when looking at the percentages, the data that
the model should predict on contains more unretained customers than retained. So
the drop from $95$ \% to  $82$ \% in unretained accuracy represents a lot more
users that the increase in retained accuracy from  $66.666$ \% to  $82$ \% when
referencing the distribution in Table \ref{tab:datasetretention}. This means
that this model will misclassify a lot more unretained users as retained
compared to the improved ability to correctly classify retained customers as
retained.
